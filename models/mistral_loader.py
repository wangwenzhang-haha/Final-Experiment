# æ–‡ä»¶: models/mistral_loader.py
# åŠŸèƒ½: åŠ è½½æœ¬åœ° Mistral-7B-Instruct æ¨¡å‹ï¼ˆ4bit é‡åŒ–ï¼‰

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

class MistralLLM:
    def __init__(self, model_id="mistralai/Mistral-7B-Instruct-v0.1"):
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨ç­‰...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def generate(self, prompt, max_tokens=150):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(**inputs, max_new_tokens=max_tokens, do_sample=True, top_p=0.95)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "")
