"""åŠ è½½æœ¬åœ°é‡åŒ– Mistral-7B-Instruct æ¨¡å‹çš„å·¥å…·ã€‚"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch


class MistralLLM:
    """å¯¹ 4-bit Mistral æ¨¡å‹çš„è½»é‡å°è£…ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬ã€‚"""

    def __init__(self, model_id="mistralai/Mistral-7B-Instruct-v0.1"):
        # Configure bitsandbytes to keep the footprint small enough for demos.
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨ç­‰...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def generate(self, prompt, max_tokens=150):
        """æ ¹æ®æç¤ºç”Ÿæˆæ–‡æœ¬ï¼Œå¹¶ç§»é™¤æ¨¡å‹å›æ˜¾çš„æç¤ºéƒ¨åˆ†ã€‚"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(**inputs, max_new_tokens=max_tokens, do_sample=True, top_p=0.95)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "")
