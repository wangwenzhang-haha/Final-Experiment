# 目标完成度复盘（按当前仓库状态）

本节对照项目最初的目标（交互加载 → 基线推荐 → 证据检索 → LLM 解释 → 可追溯 JSON 输出）进行核查，并指出仍未覆盖的里程碑。

## 已覆盖的核心目标
- **数据加载与自动补齐**：`scripts/run_demo.py` 会校验 demo 配置、确保原始交互/物品文件存在，不足时自动调用生成脚本，再构建融合图以供后续图检索使用。`DatasetLoader` 负责读取交互、物品及融合图，缺失时抛出明确错误或触发自动构建。 
- **基线推荐**：`PopularityRecommender` 使用全量交互的物品计数生成 Top-K 列表，并过滤用户已看物品，覆盖了最小可运行的推荐阶段。 
- **证据检索（向量 + 图）**：`VectorEvidenceRetriever` 依据物品词袋描述计算相似度并过滤已看物品；`GraphEvidenceFinder` 在融合图上查找用户到候选物品的短路径，保证解释可引用的节点/边。 
- **LLM 提示与解释生成**：`build_explanation_prompt` 将用户画像、候选物品描述与路径文本组合成受控提示；`ExplanationGenerator` 先尝试调用 LLM，失败时回退到可复现的本地解释，并校验 JSON 结构。流水线最终组装 `PipelineOutput`（推荐列表、证据、解释）并以 JSONL 持久化。 
- **一键演示入口**：`scripts/run_demo.py --dataset toy --k 10 --explain` 跑通数据生成→预处理→推荐→检索→解释→保存输出的闭环。

## 尚未完成/待补强的里程碑
- **模型与数据拆分**：当前仅有流行度基线，未实现 MF/LightGCN 训练或 train/val/test 拆分，所有交互直接用于推荐。后续需加入训练/评估脚本并支持可配置数据划分。 
- **离线评测**：`scripts/evaluate.py` 仍是占位文件，未计算 Recall/NDCG 或解释一致性指标，需要补充评测流程与示例指标。 
- **解释质量与鲁棒性**：提示与解析虽限制输出格式，但缺少事实校验、路径去重及对复杂场景的容错，仍需完善。 
- **配置与数据多样性**：仅提供 toy 数据与 demo 配置，尚未支持 Amazon/Douban 等真实数据的加载与可选 metadata/KG 配置。 

## 结论
当前仓库已满足“可跑通最小闭环、输出可追溯 JSON 解释”的核心目标，但距离“多模型支持、真实数据集、系统化评测”的里程碑仍有差距。建议优先补充训练/评测脚本、数据划分与更健壮的解释校验，再扩展到真实数据集与更丰富的检索策略。
